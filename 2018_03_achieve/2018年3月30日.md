- **PCDP:** While async and finish notations are useful algorithmic/pseudocode notations, we also provide you access to a high-level open-source Java-8 library called PCDP (for Parallel, Concurrent, and Distributed Programming), for which the source code is available at https://github.com/habanero-rice/pcdp. PCDP contains APIs (application programming interfaces) that directly support async and finish constructs so that you can use them in real code as well.  
- **devide and conquer（分治法）：** https://baike.baidu.com/item/%E5%88%86%E6%B2%BB%E6%B3%95/2407337?fr=aladdin  


- **Computation Graphs, Work, Span, Ideal Parallelism**  
 *Computation Graphs* (CGs), which model the execution of a parallel program as a partially ordered set. Specifically, a CG consists of:  
A set of vertices or nodes, in which each node represents a step consisting of an arbitrary sequential computation.
A set of directed edges that represent ordering constraints among steps.
For fork–join programs, it is useful to partition the edges into three cases:
> 1. **Continue edges** that capture sequencing of steps within a task.
> 2. **Fork edges** that connect a fork operation to the first step of child tasks.
>3. **Join edges** that connect the last step of a task to all join operations on that task. （分支CG图规划）

CGs can be used to define **data races**, an important class of bugs in parallel programs. We say that a data race occurs on location L in a computation graph, G, if there exist steps S1 and S2 in G such that there is no path of directed edges from S1 to S2 or from S2 to S1 in G, and both S1 and S2 read or write L (with at least one of the accesses being a write, since two parallel reads do not pose a problem).（数据竞争）

CGs can also be used to reason about the ideal parallelism of a parallel program as follows:

**Define WORK(G)** to be the sum of the execution times of all nodes in CG G,
**Define SPAN(G)** to be the length of a longest path in G, when adding up the execution times of all nodes in the path. The longest paths are known as critical paths, so SPAN also represents the critical path length (CPL) of G.
Given the above definitions of WORK and SPAN, we define the ideal parallelism of Computation Graph G as the ratio, **WORK(G)/SPAN(G)**. The ideal parallelism is an upper limit on the speedup factor that can be obtained from parallel execution of nodes in computation graph G. Note that ideal parallelism is only a function of the parallel program, and does not depend on the actual parallelism available in a physical computer. （实际工作时间与并行最理想化工作时间）

**Multiprocessor Scheduling, Parallel Speedup**  
we studied the possible executions of a Computation Graph (CG) on an idealized parallel machine with P processors. It is idealized because all processors are assumed to be identical, and the execution time of a node is assumed to be fixed, regardless of which processor it executes on. A legal schedule is one that **obeys the dependence constraints** in the CG, such that for every directed edge (A, B), the schedule guarantees that step B is only scheduled after step A completes.**Unless other specified, we will restrict our attention in this course to schedules that have no unforced idleness, i.e., schedules in which a processor is not permitted to be idle if a CG node is available to be scheduled on it. Such schedules are also referred to as *++greedy++* schedules.**（贪心算法）  
We defined TP as the execution time of a CG on P processors, and observed that  
**T∞ ≤ TP ≤ T1** 
We also saw examples for which there could be different values of TP for different schedules of the same CG on P processors.

We then defined the parallel speedup for a given schedule of a CG on P processors as **Speedup(P) = T1/TP**, and observed that **Speedup(P) must be ≤ the number of processors P**, **and also ≤ the ideal parallelism,** **WORK/SPAN.**(考虑核或程序切换性能损失下的多核比单核的加速关系)  

 **Amdahl’s Law**  
 if q ≤ 1 is the fraction of WORK in a parallel program that must be executed sequentially, then the best speedup that can be obtained for that program for any number of processors, P , is **Speedup(P) ≤ 1/q**.  
This observation follows directly from a lower bound on parallel execution time that you are familiar with, namely TP ≥ SPAN(G). If fraction q of WORK(G) is sequential, it must be the case that SPAN(G) ≥ q × WORK(G). Therefore, Speedup(P) = T1/TP must be ≤ WORK(G)/(q × WORK(G)) = 1/q since T1 = WORK(G) for greedy schedulers.  
Amdahl’s Law reminds us to watch out for sequential bottlenecks both when designing parallel algorithms and when implementing programs on real machines. As an example, if q = 10%, then Amdahl's Law reminds us that the best possible speedup must be ≤ 10 (which equals 1/q ), regardless of the number of processors available.
（并行程序性能受限于程序中相互依赖只能串行执行的片段的执行时间-多核无用论支持）